{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVbdEWnyRnwc",
        "outputId": "dcca3acc-985d-482d-b0fb-9fa0f8c98917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 27 16:58:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "--- Descargando y extrayendo datos de CIFAR-10 (formato binario) ---\n",
            "Datos descargados y extraídos en la carpeta 'cifar-10-batches-bin'.\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "print(\"\\n--- Descargando y extrayendo datos de CIFAR-10 (formato binario) ---\")\n",
        "!wget -q -c https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
        "!tar -xf cifar-10-binary.tar.gz\n",
        "print(\"Datos descargados y extraídos en la carpeta 'cifar-10-batches-bin'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mlp.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <fstream>\n",
        "#include <random>\n",
        "#include <algorithm>\n",
        "#include <iomanip>\n",
        "#include <stdexcept>\n",
        "#include <cmath>\n",
        "#include <sstream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <unordered_map>\n",
        "#include <unordered_set>\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "vector<double> dropout(const vector<double>& x, double drop_prob) {\n",
        "    static random_device rd;\n",
        "    static mt19937 gen(rd());\n",
        "    uniform_real_distribution<double> dist(0.0, 1.0);\n",
        "\n",
        "    vector<double> out(x.size());\n",
        "\n",
        "    for (size_t i = 0; i < x.size(); i++) {\n",
        "        double r = dist(gen);\n",
        "        if (r < drop_prob)\n",
        "            out[i] = 0.0;  // apagado\n",
        "        else\n",
        "            out[i] = x[i] / (1.0 - drop_prob);  // escala para mantener E[x]\n",
        "    }\n",
        "\n",
        "    return out;\n",
        "}\n",
        "\n",
        "\n",
        "vector<double> rotate90(const vector<double>& img) {\n",
        "    vector<double> out(3072);\n",
        "    for (int c = 0; c < 3; c++) {\n",
        "        for (int y = 0; y < 32; y++) {\n",
        "            for (int x = 0; x < 32; x++) {\n",
        "                int nx = 31 - y;\n",
        "                int ny = x;\n",
        "                out[c*1024 + ny*32 + nx] = img[c*1024 + y*32 + x];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return out;\n",
        "}\n",
        "\n",
        "vector<double> flip_horizontal(const vector<double>& img) {\n",
        "    vector<double> out(3072);\n",
        "    for (int c = 0; c < 3; c++) {\n",
        "        for (int y = 0; y < 32; y++) {\n",
        "            for (int x = 0; x < 32; x++) {\n",
        "                int nx = 31 - x;\n",
        "                out[c*1024 + y*32 + nx] = img[c*1024 + y*32 + x];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return out;\n",
        "}\n",
        "\n",
        "vector<double> brightness_jitter(const vector<double>& img) {\n",
        "    static random_device rd;\n",
        "    static mt19937 gen(rd());\n",
        "    uniform_real_distribution<double> dist(0.8, 1.2);\n",
        "\n",
        "    double factor = dist(gen);\n",
        "    vector<double> out(3072);\n",
        "\n",
        "    for (int i = 0; i < 3072; i++) {\n",
        "        double v = img[i] * factor;\n",
        "        out[i] = max(-1.0, min(1.0, v));\n",
        "    }\n",
        "    return out;\n",
        "}\n",
        "\n",
        "vector<double> add_noise(const vector<double>& img) {\n",
        "    static random_device rd;\n",
        "    static mt19937 gen(rd());\n",
        "    normal_distribution<double> dist(0.0, 0.05);\n",
        "\n",
        "    vector<double> out(3072);\n",
        "    for (int i = 0; i < 3072; i++) {\n",
        "        out[i] = img[i] + dist(gen);\n",
        "        out[i] = max(-1.0, min(1.0, out[i]));\n",
        "    }\n",
        "    return out;\n",
        "}\n",
        "vector<double> zoom_in(const vector<double>& img) {\n",
        "    static random_device rd;\n",
        "    static mt19937 gen(rd());\n",
        "    uniform_int_distribution<int> crop_dist(24, 30); // tamaño del recorte\n",
        "\n",
        "    int crop = crop_dist(gen);\n",
        "    int offset = 32 - crop;\n",
        "\n",
        "    uniform_int_distribution<int> off_dist(0, offset);\n",
        "    int ox = off_dist(gen);\n",
        "    int oy = off_dist(gen);\n",
        "\n",
        "    // recorte\n",
        "    vector<double> cropped(crop * crop * 3);\n",
        "\n",
        "    for (int c = 0; c < 3; c++) {\n",
        "        for (int y = 0; y < crop; y++) {\n",
        "            for (int x = 0; x < crop; x++) {\n",
        "                int src_x = x + ox;\n",
        "                int src_y = y + oy;\n",
        "                cropped[c*crop*crop + y*crop + x] =\n",
        "                    img[c*1024 + src_y*32 + src_x];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // reescalar a 32x32 (nearest neighbor)\n",
        "    vector<double> out(3072);\n",
        "    for (int c = 0; c < 3; c++) {\n",
        "        for (int y = 0; y < 32; y++) {\n",
        "            for (int x = 0; x < 32; x++) {\n",
        "                int src_x = x * crop / 32;\n",
        "                int src_y = y * crop / 32;\n",
        "                out[c*1024 + y*32 + x] =\n",
        "                    cropped[c * crop * crop + src_y * crop + src_x];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return out;\n",
        "}\n",
        "\n",
        "vector<double> shift_image(const vector<double>& img) {\n",
        "    static random_device rd;\n",
        "    static mt19937 gen(rd());\n",
        "    uniform_int_distribution<int> dist(-3, 3);\n",
        "\n",
        "    int dx = dist(gen);\n",
        "    int dy = dist(gen);\n",
        "\n",
        "    vector<double> out(3072, 0.0);\n",
        "\n",
        "    for (int c = 0; c < 3; c++) {\n",
        "        for (int y = 0; y < 32; y++) {\n",
        "            int ny = y + dy;\n",
        "            if (ny < 0 || ny >= 32) continue;\n",
        "\n",
        "            for (int x = 0; x < 32; x++) {\n",
        "                int nx = x + dx;\n",
        "                if (nx < 0 || nx >= 32) continue;\n",
        "\n",
        "                out[c*1024 + ny*32 + nx] =\n",
        "                    img[c*1024 + y*32 + x];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return out;\n",
        "}\n",
        "\n",
        "vector<double> augment(const vector<double>& img) {\n",
        "    static random_device rd;\n",
        "    static mt19937 gen(rd());\n",
        "    uniform_int_distribution<int> dist(0, 5);\n",
        "\n",
        "    int op = dist(gen);\n",
        "\n",
        "    switch (op) {\n",
        "        case 0: return flip_horizontal(img);\n",
        "        case 1: return rotate90(img);\n",
        "        case 2: return brightness_jitter(img);\n",
        "        case 3: return add_noise(img);\n",
        "        case 4: return zoom_in(img);\n",
        "        case 5: return shift_image(img);\n",
        "    }\n",
        "    return img;\n",
        "}\n",
        "\n",
        "__host__ __device__ inline double relu(double x) {\n",
        "    return x > 0 ? x : 0;\n",
        "}\n",
        "\n",
        "__host__ __device__ inline double relu_derivative(double x) {\n",
        "    return x > 0 ? 1.0 : 0.0;\n",
        "}\n",
        "\n",
        "__global__ void softmax_kernel(const double* logits, double* soft, int n) {\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // 1. Compute max\n",
        "    __shared__ double maxv;\n",
        "    if (tid == 0) {\n",
        "        maxv = logits[0];\n",
        "        for (int i = 1; i < n; i++)\n",
        "            maxv = fmax(maxv, logits[i]);\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // 2. Compute exp(x - max)\n",
        "    __shared__ double sum;\n",
        "    if (tid == 0) sum = 0.0;\n",
        "    __syncthreads();\n",
        "\n",
        "    double val = 0;\n",
        "    if (tid < n) {\n",
        "        val = exp(logits[tid] - maxv);\n",
        "        atomicAdd(&sum, val);\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // 3. Normalize\n",
        "    if (tid < n) {\n",
        "        soft[tid] = val / sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void ce_grad_kernel(const double* soft, double* dZ, int label, int n) {\n",
        "    int i = threadIdx.x;\n",
        "    if (i < n) dZ[i] = soft[i] - (i == label);\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void forward_kernel(const double* __restrict__ W,\n",
        "    const double* __restrict__ b, const double* __restrict__ input,\n",
        "    double* __restrict__ output, int input_size, int output_size,\n",
        "    bool apply_relu) {\n",
        "\n",
        "    int i = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (i >= output_size) return;\n",
        "\n",
        "    __shared__ double sum[BLOCK_SIZE];\n",
        "    sum[tid] = 0.0;\n",
        "\n",
        "    for (int j = tid; j < input_size; j += blockDim.x) {\n",
        "        sum[tid] += W[i * input_size + j] * input[j];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sum[tid] += sum[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        double total = b[i] + sum[0];\n",
        "        output[i] = apply_relu ? relu(total) : total;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__global__ void get_grad_input_kernel(const double* __restrict__ W,\n",
        "    const double* __restrict__ grad_out, double* __restrict__ grad_in,\n",
        "    int input_size, int output_size) {\n",
        "\n",
        "    int j = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    if (j >= input_size) return;\n",
        "\n",
        "    __shared__ double sum[BLOCK_SIZE];\n",
        "    sum[tid] = 0.0;\n",
        "\n",
        "    for (int i = tid; i < output_size; i += blockDim.x) {\n",
        "        sum[tid] += W[i * input_size + j] * grad_out[i];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sum[tid] += sum[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) grad_in[j] = sum[0];\n",
        "}\n",
        "\n",
        "__global__ void update_weights_kernel(double* __restrict__ W, double* __restrict__ b,\n",
        "    const double* __restrict__ grad_out, const double* __restrict__ input,\n",
        "    int input_size, int output_size, double lr) {\n",
        "\n",
        "    int i = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (i >= output_size) return;\n",
        "\n",
        "    for (int col = tid; col < input_size; col += blockDim.x) {\n",
        "        W[i * input_size + col] -= lr * grad_out[i] * input[col];\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        b[i] -= lr * grad_out[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void relu_backward_kernel(const double* grad_out,\n",
        "    const double* activ, double* grad_in, int n) {\n",
        "\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i >= n) return;\n",
        "\n",
        "    grad_in[i] = grad_out[i] * relu_derivative(activ[i]);\n",
        "}\n",
        "\n",
        "struct DenseLayer {\n",
        "    int input_size, output_size;\n",
        "    vector<double> h_W, h_b;\n",
        "    double *d_W, *d_b, *d_input, *d_output;\n",
        "    DenseLayer() : input_size(0), output_size(0),\n",
        "                   d_W(nullptr), d_b(nullptr), d_input(nullptr), d_output(nullptr) {}\n",
        "\n",
        "    DenseLayer(int in_size, int out_size) {\n",
        "        init(in_size, out_size);\n",
        "    }\n",
        "\n",
        "    void init(int in_size, int out_size) {\n",
        "        input_size = in_size;\n",
        "        output_size = out_size;\n",
        "        h_W.assign(out_size * in_size, 0.0);\n",
        "        h_b.assign(out_size, 0.0);\n",
        "\n",
        "        random_device rd;\n",
        "        mt19937 gen(rd());\n",
        "        normal_distribution<double> dist(0.0, sqrt(2.0 / input_size));\n",
        "        for (auto &w : h_W) w = dist(gen);\n",
        "\n",
        "        cudaMalloc(&d_W, sizeof(double) * out_size * in_size);\n",
        "        cudaMalloc(&d_b, sizeof(double) * out_size);\n",
        "        cudaMalloc(&d_input, sizeof(double) * in_size);\n",
        "        cudaMalloc(&d_output, sizeof(double) * out_size);\n",
        "\n",
        "        cudaMemcpy(d_W, h_W.data(), sizeof(double) * out_size * in_size, cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_b, h_b.data(), sizeof(double) * out_size, cudaMemcpyHostToDevice);\n",
        "    }\n",
        "\n",
        "    ~DenseLayer() {\n",
        "        if (d_W) cudaFree(d_W);\n",
        "        if (d_b) cudaFree(d_b);\n",
        "        if (d_input) cudaFree(d_input);\n",
        "        if (d_output) cudaFree(d_output);\n",
        "    }\n",
        "\n",
        "    vector<double> forward(const vector<double>& input, bool apply_relu) {\n",
        "        cudaMemcpy(d_input, input.data(), sizeof(double) * input_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "        forward_kernel<<<output_size, BLOCK_SIZE>>>(d_W, d_b, d_input, d_output,\n",
        "            input_size, output_size, apply_relu);\n",
        "\n",
        "        cudaDeviceSynchronize();\n",
        "        vector<double> out(output_size);\n",
        "        cudaMemcpy(out.data(), d_output, sizeof(double) * output_size, cudaMemcpyDeviceToHost);\n",
        "        return out;\n",
        "    }\n",
        "\n",
        "\n",
        "    vector<double> backward(const vector<double>& input, const vector<double>& grad_out, double lr) {\n",
        "        double *d_grad_out, *d_grad_in;\n",
        "        cudaMalloc(&d_grad_out, sizeof(double) * output_size);\n",
        "        cudaMalloc(&d_grad_in, sizeof(double) * input_size);\n",
        "\n",
        "        cudaMemcpy(d_grad_out, grad_out.data(), sizeof(double) * output_size, cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_input, input.data(), sizeof(double) * input_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "        get_grad_input_kernel<<<input_size, BLOCK_SIZE>>>(d_W, d_grad_out, d_grad_in, input_size, output_size);\n",
        "\n",
        "        update_weights_kernel<<<output_size, BLOCK_SIZE>>>(d_W, d_b, d_grad_out, d_input, input_size, output_size, lr);\n",
        "\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        vector<double> grad_in(input_size);\n",
        "        cudaMemcpy(grad_in.data(), d_grad_in, sizeof(double) * input_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        cudaFree(d_grad_out);\n",
        "        cudaFree(d_grad_in);\n",
        "        return grad_in;\n",
        "    }\n",
        "};\n",
        "\n",
        "struct MLP {\n",
        "    DenseLayer l1, l2, out;\n",
        "    double lr;\n",
        "    float prob_drop = 0.3;\n",
        "    MLP(int input_size, int h1, int h2, int output_size, double lr_) : lr(lr_) {\n",
        "        l1.init(input_size, h1);\n",
        "        l2.init(h1, h2);\n",
        "        out.init(h2, output_size);\n",
        "    }\n",
        "\n",
        "    vector<double> apply_relu_backward_cuda(const vector<double>& grad_out,\n",
        "        const vector<double>& activ){\n",
        "\n",
        "        int n = grad_out.size();\n",
        "\n",
        "        double *d_grad_out, *d_activ, *d_grad_in;\n",
        "        cudaMalloc(&d_grad_out, sizeof(double)*n);\n",
        "        cudaMalloc(&d_activ,    sizeof(double)*n);\n",
        "        cudaMalloc(&d_grad_in,  sizeof(double)*n);\n",
        "\n",
        "        cudaMemcpy(d_grad_out, grad_out.data(), sizeof(double)*n, cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_activ,    activ.data(),    sizeof(double)*n, cudaMemcpyHostToDevice);\n",
        "\n",
        "        int threads = 256;\n",
        "        int blocks = (n + threads - 1) / threads;\n",
        "        relu_backward_kernel<<<blocks, threads>>>(d_grad_out, d_activ, d_grad_in, n);\n",
        "\n",
        "        vector<double> grad_in(n);\n",
        "        cudaMemcpy(grad_in.data(), d_grad_in, sizeof(double)*n, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        cudaFree(d_grad_out);\n",
        "        cudaFree(d_activ);\n",
        "        cudaFree(d_grad_in);\n",
        "\n",
        "        return grad_in;\n",
        "    }\n",
        "\n",
        "    vector<double> forward(const vector<double>& x, vector<double>& a1_out, vector<double>& a2_out, bool training) {\n",
        "\n",
        "        a1_out = l1.forward(x, true);\n",
        "        if (training)\n",
        "            a1_out = dropout(a1_out, prob_drop);\n",
        "\n",
        "        a2_out = l2.forward(a1_out, true);\n",
        "        if (training)\n",
        "            a2_out = dropout(a2_out, prob_drop);\n",
        "\n",
        "        vector<double> logits = out.forward(a2_out, false);\n",
        "\n",
        "        int C = logits.size();\n",
        "        double *d_logits, *d_soft;\n",
        "\n",
        "        cudaMalloc(&d_logits, sizeof(double)*C);\n",
        "        cudaMalloc(&d_soft,   sizeof(double)*C);\n",
        "        cudaMemcpy(d_logits, logits.data(), sizeof(double)*C, cudaMemcpyHostToDevice);\n",
        "\n",
        "        softmax_kernel<<<1, C>>>(d_logits, d_soft, C);\n",
        "\n",
        "        vector<double> soft(C);\n",
        "        cudaMemcpy(soft.data(), d_soft, sizeof(double)*C, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        cudaFree(d_logits);\n",
        "        cudaFree(d_soft);\n",
        "\n",
        "        return soft;\n",
        "    }\n",
        "\n",
        "\n",
        "    vector<double> backward(const vector<double>& x,\n",
        "                    const vector<double>& a1,\n",
        "                    const vector<double>& a2,\n",
        "                    const vector<double>& soft,\n",
        "                    const vector<double>& target) {\n",
        "\n",
        "        int C = soft.size();\n",
        "        double *d_soft, *d_dZ;\n",
        "        cudaMalloc(&d_soft, sizeof(double)*C);\n",
        "        cudaMalloc(&d_dZ,   sizeof(double)*C);\n",
        "\n",
        "        cudaMemcpy(d_soft, soft.data(), sizeof(double)*C, cudaMemcpyHostToDevice);\n",
        "\n",
        "        int label = -1;\n",
        "        for (int i = 0; i < C; i++)\n",
        "            if (target[i] == 1.0) label = i;\n",
        "\n",
        "        ce_grad_kernel<<<1, C>>>(d_soft, d_dZ, label, C);\n",
        "\n",
        "        vector<double> grad_out(C);\n",
        "        cudaMemcpy(grad_out.data(), d_dZ, sizeof(double)*C, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        cudaFree(d_soft);\n",
        "        cudaFree(d_dZ);\n",
        "\n",
        "        vector<double> grad_a2 = out.backward(a2, grad_out, lr);\n",
        "\n",
        "        vector<double> grad_out_a2 = apply_relu_backward_cuda(grad_a2, a2);\n",
        "        vector<double> grad_a1 = l2.backward(a1, grad_out_a2, lr);\n",
        "\n",
        "        vector<double> grad_out_a1 = apply_relu_backward_cuda(grad_a1, a1);\n",
        "        return l1.backward(x, grad_out_a1, lr);\n",
        "    }\n",
        "\n",
        "    double compute_loss(const vector<double>& pred, int label) {\n",
        "        return -log(max(pred[label], 1e-12));\n",
        "    }\n",
        "\n",
        "\n",
        "    int predict(const vector<double>& x) {\n",
        "        vector<double> a1, a2;\n",
        "        auto outv = forward(x, a1, a2, false);\n",
        "        return int(max_element(outv.begin(), outv.end()) - outv.begin());\n",
        "    }\n",
        "\n",
        "    double evaluate(const vector<vector<double>>& X, const vector<int>& y) {\n",
        "        int correct = 0;\n",
        "        for (int i = 0; i < (int)X.size(); i++) {\n",
        "            if (predict(X[i]) == y[i]) correct++;\n",
        "        }\n",
        "        return (double)correct / X.size() * 100.0;\n",
        "    }\n",
        "\n",
        "    void fit(const vector<vector<double>>& X, const vector<int>& y,\n",
        "             int epochs, int batch_size,\n",
        "             const vector<vector<double>>& X_val = {},\n",
        "             const vector<int>& y_val = {}) {\n",
        "\n",
        "        int n = (int)X.size();\n",
        "        vector<int> indices(n);\n",
        "        iota(indices.begin(), indices.end(), 0);\n",
        "\n",
        "        ofstream log_file(\"loss_history.txt\");\n",
        "        log_file << \"epoch,train_loss,val_loss\\n\";\n",
        "\n",
        "        int num_batches = (n + batch_size - 1) / batch_size;\n",
        "\n",
        "        for (int epoch = 0; epoch < epochs; epoch++) {\n",
        "            random_shuffle(indices.begin(), indices.end());\n",
        "            double total_loss = 0.0;\n",
        "\n",
        "            for (int start = 0; start < n; start += batch_size) {\n",
        "                int end = min(start + batch_size, n);\n",
        "                double batch_loss = 0.0;\n",
        "\n",
        "                for (int idx_pos = start; idx_pos < end; idx_pos++) {\n",
        "                    int idx = indices[idx_pos];\n",
        "                    vector<double> a1, a2;\n",
        "                    vector<double> xin = augment(X[idx]);\n",
        "                    auto pred = forward(xin, a1, a2, true);\n",
        "\n",
        "                    vector<double> target(pred.size(), 0.0);\n",
        "                    target[y[idx]] = 1.0;\n",
        "\n",
        "                    batch_loss += compute_loss(pred, y[idx]);\n",
        "                    backward(xin, a1, a2, pred, target);\n",
        "                }\n",
        "                total_loss += batch_loss / (end - start);\n",
        "                cudaDeviceSynchronize();\n",
        "            }\n",
        "\n",
        "            double avg_train_loss = total_loss / num_batches;\n",
        "            double val_loss = 0.0;\n",
        "\n",
        "            if (!X_val.empty()) {\n",
        "                for (int i = 0; i < (int)X_val.size(); i++) {\n",
        "                    vector<double> a1, a2;\n",
        "                    auto pred_val = forward(X_val[i], a1, a2,false);\n",
        "                    val_loss += compute_loss(pred_val, y_val[i]);\n",
        "                }\n",
        "                val_loss /= X_val.size();\n",
        "            }\n",
        "\n",
        "            cout << \"Epoch \" << epoch + 1\n",
        "                 << \" - Train Loss: \" << fixed << setprecision(5) << avg_train_loss;\n",
        "            if (!X_val.empty())\n",
        "                cout << \" - Val Loss: \" << val_loss;\n",
        "            cout << endl;\n",
        "\n",
        "            log_file << epoch + 1 << \",\" << avg_train_loss << \",\";\n",
        "            if (!X_val.empty()) log_file << val_loss;\n",
        "            log_file << \"\\n\";\n",
        "        }\n",
        "\n",
        "        log_file.close();\n",
        "    }\n",
        "};\n",
        "\n",
        "void load_cifar10_batch(const string& filename, vector<vector<double>>& images, vector<int>& labels) {\n",
        "    ifstream file(filename, ios::binary);\n",
        "    if (!file.is_open()) throw runtime_error(\"No se pudo abrir \" + filename);\n",
        "\n",
        "    const int num_images = 10000;\n",
        "    const int image_size = 3072; // 32x32x3\n",
        "    for (int i = 0; i < num_images; i++) {\n",
        "        unsigned char label;\n",
        "        file.read((char*)&label, 1);\n",
        "        labels.push_back((int)label);\n",
        "\n",
        "        vector<double> img(image_size);\n",
        "        unsigned char buffer[image_size];\n",
        "        file.read((char*)buffer, image_size);\n",
        "\n",
        "        for (int j = 0; j < image_size; j++)\n",
        "            img[j] = (buffer[j] / 255.0 - 0.5) * 2.0;\n",
        "\n",
        "        images.push_back(std::move(img));\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    vector<vector<double>> train_images, test_images;\n",
        "    vector<int> train_labels, test_labels;\n",
        "\n",
        "    for (int i = 1; i <= 5; i++) {\n",
        "        string fname = \"cifar-10-batches-bin/data_batch_\" + to_string(i) + \".bin\";\n",
        "        load_cifar10_batch(fname, train_images, train_labels);\n",
        "    }\n",
        "    load_cifar10_batch(\"cifar-10-batches-bin/test_batch.bin\", test_images, test_labels);\n",
        "\n",
        "    cout << \"Cargadas \" << train_images.size() << \" train y \" << test_images.size() << \" test\" << endl;\n",
        "\n",
        "    int input_size = 32 * 32 * 3;\n",
        "    int h1 = 512;\n",
        "    int h2 = 128;\n",
        "    int output_size = 10;\n",
        "    int epochs = 500;\n",
        "    int minibatch_size = 250;\n",
        "    double learning_rate = 0.0001;\n",
        "\n",
        "    MLP model(input_size, h1, h2, output_size, learning_rate);\n",
        "\n",
        "    model.fit(train_images, train_labels, epochs, minibatch_size, test_images, test_labels);\n",
        "    //model.fit(train_images, train_labels, epochs, minibatch_size);\n",
        "\n",
        "    double train_acc = model.evaluate(train_images, train_labels);\n",
        "    double test_acc = model.evaluate(test_images, test_labels);\n",
        "\n",
        "    cout << \"Train acc: \" << train_acc << \"%\" << endl;\n",
        "    cout << \"Test acc: \" << test_acc << \"%\" << endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzjRHhkpRpWW",
        "outputId": "a85de545-1d38-4e2b-82fa-cc02746183e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlp.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Compilando el código CUDA (mlp.cu) ---\")\n",
        "!nvcc mlp.cu -o mlp_executable -arch=sm_75\n",
        "print(\"Compilación finalizada. Se ha creado 'mlp_executable'.\")\n",
        "\n",
        "print(\"\\n--- ¡Iniciando el entrenamiento del MLP! ---\")\n",
        "!./mlp_executable\n",
        "print(\"\\n--- Entrenamiento completado. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_oqoDZdRxcF",
        "outputId": "e271cb99-7294-4753-9b18-f90dbb39b3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Compilando el código CUDA (mlp.cu) ---\n",
            "Compilación finalizada. Se ha creado 'mlp_executable'.\n",
            "\n",
            "--- ¡Iniciando el entrenamiento del MLP! ---\n",
            "Cargadas 50000 train y 10000 test\n",
            "Epoch 1 - Train Loss: 2.13674 - Val Loss: 1.83766\n",
            "Epoch 2 - Train Loss: 1.95536 - Val Loss: 1.75788\n",
            "Epoch 3 - Train Loss: 1.89593 - Val Loss: 1.71094\n",
            "Epoch 4 - Train Loss: 1.85644 - Val Loss: 1.67689\n",
            "Epoch 5 - Train Loss: 1.82651 - Val Loss: 1.64629\n",
            "Epoch 6 - Train Loss: 1.80503 - Val Loss: 1.62640\n",
            "Epoch 7 - Train Loss: 1.79126 - Val Loss: 1.60760\n",
            "Epoch 8 - Train Loss: 1.77216 - Val Loss: 1.59237\n",
            "Epoch 9 - Train Loss: 1.76082 - Val Loss: 1.57974\n",
            "Epoch 10 - Train Loss: 1.74769 - Val Loss: 1.56915\n",
            "Epoch 11 - Train Loss: 1.74110 - Val Loss: 1.55658\n",
            "Epoch 12 - Train Loss: 1.72627 - Val Loss: 1.54669\n",
            "Epoch 13 - Train Loss: 1.71547 - Val Loss: 1.53571\n",
            "Epoch 14 - Train Loss: 1.70758 - Val Loss: 1.52826\n",
            "Epoch 15 - Train Loss: 1.69747 - Val Loss: 1.51605\n",
            "Epoch 16 - Train Loss: 1.69078 - Val Loss: 1.50784\n",
            "Epoch 17 - Train Loss: 1.68621 - Val Loss: 1.50104\n",
            "Epoch 18 - Train Loss: 1.67765 - Val Loss: 1.49700\n",
            "Epoch 19 - Train Loss: 1.67121 - Val Loss: 1.48730\n",
            "Epoch 20 - Train Loss: 1.66339 - Val Loss: 1.48015\n",
            "Epoch 21 - Train Loss: 1.66067 - Val Loss: 1.47460\n",
            "Epoch 22 - Train Loss: 1.65329 - Val Loss: 1.47321\n",
            "Epoch 23 - Train Loss: 1.64828 - Val Loss: 1.46259\n",
            "Epoch 24 - Train Loss: 1.64199 - Val Loss: 1.45900\n",
            "Epoch 25 - Train Loss: 1.63648 - Val Loss: 1.45420\n",
            "Epoch 26 - Train Loss: 1.62659 - Val Loss: 1.44781\n",
            "Epoch 27 - Train Loss: 1.62867 - Val Loss: 1.44451\n",
            "Epoch 28 - Train Loss: 1.62093 - Val Loss: 1.44092\n",
            "Epoch 29 - Train Loss: 1.61789 - Val Loss: 1.43544\n",
            "Epoch 30 - Train Loss: 1.60977 - Val Loss: 1.42638\n",
            "Epoch 31 - Train Loss: 1.60820 - Val Loss: 1.42742\n",
            "Epoch 32 - Train Loss: 1.60492 - Val Loss: 1.42090\n",
            "Epoch 33 - Train Loss: 1.59702 - Val Loss: 1.41823\n",
            "Epoch 34 - Train Loss: 1.59902 - Val Loss: 1.41655\n",
            "Epoch 35 - Train Loss: 1.58928 - Val Loss: 1.40965\n",
            "Epoch 36 - Train Loss: 1.58537 - Val Loss: 1.40689\n",
            "Epoch 37 - Train Loss: 1.58106 - Val Loss: 1.40198\n",
            "Epoch 38 - Train Loss: 1.57897 - Val Loss: 1.39971\n",
            "Epoch 39 - Train Loss: 1.57572 - Val Loss: 1.39634\n",
            "Epoch 40 - Train Loss: 1.57134 - Val Loss: 1.39457\n",
            "Epoch 41 - Train Loss: 1.57254 - Val Loss: 1.39168\n",
            "Epoch 42 - Train Loss: 1.56830 - Val Loss: 1.38798\n",
            "Epoch 43 - Train Loss: 1.56750 - Val Loss: 1.38435\n",
            "Epoch 44 - Train Loss: 1.55992 - Val Loss: 1.37988\n",
            "Epoch 45 - Train Loss: 1.55816 - Val Loss: 1.37657\n",
            "Epoch 46 - Train Loss: 1.55329 - Val Loss: 1.37611\n",
            "Epoch 47 - Train Loss: 1.55333 - Val Loss: 1.37262\n",
            "Epoch 48 - Train Loss: 1.54763 - Val Loss: 1.36870\n",
            "Epoch 49 - Train Loss: 1.54674 - Val Loss: 1.36289\n",
            "Epoch 50 - Train Loss: 1.53926 - Val Loss: 1.36313\n",
            "Epoch 51 - Train Loss: 1.54218 - Val Loss: 1.36101\n",
            "Epoch 52 - Train Loss: 1.53918 - Val Loss: 1.35947\n",
            "Epoch 53 - Train Loss: 1.53534 - Val Loss: 1.35591\n",
            "Epoch 54 - Train Loss: 1.53204 - Val Loss: 1.35589\n",
            "Epoch 55 - Train Loss: 1.52579 - Val Loss: 1.35225\n",
            "Epoch 56 - Train Loss: 1.52802 - Val Loss: 1.34931\n",
            "Epoch 57 - Train Loss: 1.51876 - Val Loss: 1.34650\n",
            "Epoch 58 - Train Loss: 1.51901 - Val Loss: 1.34483\n",
            "Epoch 59 - Train Loss: 1.51960 - Val Loss: 1.34077\n",
            "Epoch 60 - Train Loss: 1.51501 - Val Loss: 1.34170\n",
            "Epoch 61 - Train Loss: 1.51155 - Val Loss: 1.33977\n",
            "Epoch 62 - Train Loss: 1.50969 - Val Loss: 1.33704\n",
            "Epoch 63 - Train Loss: 1.50612 - Val Loss: 1.33398\n",
            "Epoch 64 - Train Loss: 1.50325 - Val Loss: 1.32867\n",
            "Epoch 65 - Train Loss: 1.49742 - Val Loss: 1.32774\n",
            "Epoch 66 - Train Loss: 1.49940 - Val Loss: 1.32741\n",
            "Epoch 67 - Train Loss: 1.50040 - Val Loss: 1.32349\n",
            "Epoch 68 - Train Loss: 1.49925 - Val Loss: 1.32209\n",
            "Epoch 69 - Train Loss: 1.49194 - Val Loss: 1.32215\n",
            "Epoch 70 - Train Loss: 1.49630 - Val Loss: 1.31643\n",
            "Epoch 71 - Train Loss: 1.49143 - Val Loss: 1.31692\n",
            "Epoch 72 - Train Loss: 1.48651 - Val Loss: 1.31224\n",
            "Epoch 73 - Train Loss: 1.48127 - Val Loss: 1.30925\n",
            "Epoch 74 - Train Loss: 1.47865 - Val Loss: 1.30837\n",
            "Epoch 75 - Train Loss: 1.47651 - Val Loss: 1.30558\n",
            "Epoch 76 - Train Loss: 1.47223 - Val Loss: 1.30544\n",
            "Epoch 77 - Train Loss: 1.47242 - Val Loss: 1.30290\n",
            "Epoch 78 - Train Loss: 1.47277 - Val Loss: 1.29915\n",
            "Epoch 79 - Train Loss: 1.47172 - Val Loss: 1.29963\n",
            "Epoch 80 - Train Loss: 1.47183 - Val Loss: 1.29744\n",
            "Epoch 81 - Train Loss: 1.46535 - Val Loss: 1.29585\n",
            "Epoch 82 - Train Loss: 1.46677 - Val Loss: 1.29604\n",
            "Epoch 83 - Train Loss: 1.46643 - Val Loss: 1.29051\n",
            "Epoch 84 - Train Loss: 1.45738 - Val Loss: 1.28825\n",
            "Epoch 85 - Train Loss: 1.46275 - Val Loss: 1.28627\n",
            "Epoch 86 - Train Loss: 1.45871 - Val Loss: 1.28602\n",
            "Epoch 87 - Train Loss: 1.45461 - Val Loss: 1.28563\n",
            "Epoch 88 - Train Loss: 1.44762 - Val Loss: 1.28153\n",
            "Epoch 89 - Train Loss: 1.45027 - Val Loss: 1.28104\n",
            "Epoch 90 - Train Loss: 1.45492 - Val Loss: 1.28005\n",
            "Epoch 91 - Train Loss: 1.45095 - Val Loss: 1.27963\n",
            "Epoch 92 - Train Loss: 1.44697 - Val Loss: 1.27757\n",
            "Epoch 93 - Train Loss: 1.44778 - Val Loss: 1.27669\n",
            "Epoch 94 - Train Loss: 1.44434 - Val Loss: 1.27547\n",
            "Epoch 95 - Train Loss: 1.44243 - Val Loss: 1.27401\n",
            "Epoch 96 - Train Loss: 1.43749 - Val Loss: 1.27199\n",
            "Epoch 97 - Train Loss: 1.43412 - Val Loss: 1.27200\n",
            "Epoch 98 - Train Loss: 1.43858 - Val Loss: 1.26841\n",
            "Epoch 99 - Train Loss: 1.43334 - Val Loss: 1.26751\n",
            "Epoch 100 - Train Loss: 1.43155 - Val Loss: 1.26601\n",
            "Epoch 101 - Train Loss: 1.42887 - Val Loss: 1.26493\n",
            "Epoch 102 - Train Loss: 1.42181 - Val Loss: 1.26091\n",
            "Epoch 103 - Train Loss: 1.43327 - Val Loss: 1.26061\n",
            "Epoch 104 - Train Loss: 1.42262 - Val Loss: 1.25802\n",
            "Epoch 105 - Train Loss: 1.42475 - Val Loss: 1.25704\n",
            "Epoch 106 - Train Loss: 1.42189 - Val Loss: 1.25779\n",
            "Epoch 107 - Train Loss: 1.42328 - Val Loss: 1.25649\n",
            "Epoch 108 - Train Loss: 1.42028 - Val Loss: 1.25434\n",
            "Epoch 109 - Train Loss: 1.41657 - Val Loss: 1.25015\n",
            "Epoch 110 - Train Loss: 1.41852 - Val Loss: 1.25130\n",
            "Epoch 111 - Train Loss: 1.41378 - Val Loss: 1.24881\n",
            "Epoch 112 - Train Loss: 1.41499 - Val Loss: 1.24906\n",
            "Epoch 113 - Train Loss: 1.41004 - Val Loss: 1.24722\n",
            "Epoch 114 - Train Loss: 1.40737 - Val Loss: 1.24824\n",
            "Epoch 115 - Train Loss: 1.40845 - Val Loss: 1.24570\n",
            "Epoch 116 - Train Loss: 1.40681 - Val Loss: 1.24349\n",
            "Epoch 117 - Train Loss: 1.40416 - Val Loss: 1.24450\n",
            "Epoch 118 - Train Loss: 1.40193 - Val Loss: 1.24310\n",
            "Epoch 119 - Train Loss: 1.39692 - Val Loss: 1.23999\n",
            "Epoch 120 - Train Loss: 1.39896 - Val Loss: 1.24181\n",
            "Epoch 121 - Train Loss: 1.39446 - Val Loss: 1.24300\n",
            "Epoch 122 - Train Loss: 1.39237 - Val Loss: 1.23634\n",
            "Epoch 123 - Train Loss: 1.39130 - Val Loss: 1.23751\n",
            "Epoch 124 - Train Loss: 1.39508 - Val Loss: 1.23840\n",
            "Epoch 125 - Train Loss: 1.39342 - Val Loss: 1.23723\n",
            "Epoch 126 - Train Loss: 1.39108 - Val Loss: 1.23214\n",
            "Epoch 127 - Train Loss: 1.38869 - Val Loss: 1.23341\n",
            "Epoch 128 - Train Loss: 1.38737 - Val Loss: 1.23233\n",
            "Epoch 129 - Train Loss: 1.38536 - Val Loss: 1.23328\n",
            "Epoch 130 - Train Loss: 1.38383 - Val Loss: 1.23244\n",
            "Epoch 131 - Train Loss: 1.38389 - Val Loss: 1.22869\n",
            "Epoch 132 - Train Loss: 1.37666 - Val Loss: 1.22655\n",
            "Epoch 133 - Train Loss: 1.38165 - Val Loss: 1.22493\n",
            "Epoch 134 - Train Loss: 1.37981 - Val Loss: 1.22880\n",
            "Epoch 135 - Train Loss: 1.37697 - Val Loss: 1.22436\n",
            "Epoch 136 - Train Loss: 1.37345 - Val Loss: 1.22114\n",
            "Epoch 137 - Train Loss: 1.37854 - Val Loss: 1.22302\n",
            "Epoch 138 - Train Loss: 1.37753 - Val Loss: 1.22017\n",
            "Epoch 139 - Train Loss: 1.36980 - Val Loss: 1.21981\n",
            "Epoch 140 - Train Loss: 1.37172 - Val Loss: 1.21995\n",
            "Epoch 141 - Train Loss: 1.37276 - Val Loss: 1.21943\n",
            "Epoch 142 - Train Loss: 1.36654 - Val Loss: 1.21889\n",
            "Epoch 143 - Train Loss: 1.36847 - Val Loss: 1.21798\n",
            "Epoch 144 - Train Loss: 1.36952 - Val Loss: 1.21598\n",
            "Epoch 145 - Train Loss: 1.36896 - Val Loss: 1.21763\n",
            "Epoch 146 - Train Loss: 1.36641 - Val Loss: 1.21392\n",
            "Epoch 147 - Train Loss: 1.36154 - Val Loss: 1.21418\n",
            "Epoch 148 - Train Loss: 1.36017 - Val Loss: 1.21489\n",
            "Epoch 149 - Train Loss: 1.36074 - Val Loss: 1.21472\n",
            "Epoch 150 - Train Loss: 1.36590 - Val Loss: 1.21148\n",
            "Epoch 151 - Train Loss: 1.35513 - Val Loss: 1.20846\n",
            "Epoch 152 - Train Loss: 1.35484 - Val Loss: 1.20596\n",
            "Epoch 153 - Train Loss: 1.35726 - Val Loss: 1.20633\n",
            "Epoch 154 - Train Loss: 1.35129 - Val Loss: 1.20726\n",
            "Epoch 155 - Train Loss: 1.35599 - Val Loss: 1.20559\n",
            "Epoch 156 - Train Loss: 1.35565 - Val Loss: 1.20528\n",
            "Epoch 157 - Train Loss: 1.34602 - Val Loss: 1.20198\n",
            "Epoch 158 - Train Loss: 1.34877 - Val Loss: 1.20146\n",
            "Epoch 159 - Train Loss: 1.34766 - Val Loss: 1.20201\n",
            "Epoch 160 - Train Loss: 1.33731 - Val Loss: 1.20018\n",
            "Epoch 161 - Train Loss: 1.34399 - Val Loss: 1.20112\n",
            "Epoch 162 - Train Loss: 1.34255 - Val Loss: 1.19840\n",
            "Epoch 163 - Train Loss: 1.34541 - Val Loss: 1.19680\n",
            "Epoch 164 - Train Loss: 1.33595 - Val Loss: 1.19707\n",
            "Epoch 165 - Train Loss: 1.34308 - Val Loss: 1.19989\n",
            "Epoch 166 - Train Loss: 1.34133 - Val Loss: 1.19859\n",
            "Epoch 167 - Train Loss: 1.34104 - Val Loss: 1.19852\n",
            "Epoch 168 - Train Loss: 1.33677 - Val Loss: 1.19459\n",
            "Epoch 169 - Train Loss: 1.33707 - Val Loss: 1.19517\n",
            "Epoch 170 - Train Loss: 1.33474 - Val Loss: 1.19456\n",
            "Epoch 171 - Train Loss: 1.33421 - Val Loss: 1.19076\n",
            "Epoch 172 - Train Loss: 1.33406 - Val Loss: 1.18967\n",
            "Epoch 173 - Train Loss: 1.33380 - Val Loss: 1.19063\n",
            "Epoch 174 - Train Loss: 1.33149 - Val Loss: 1.19204\n",
            "Epoch 175 - Train Loss: 1.32673 - Val Loss: 1.18740\n",
            "Epoch 176 - Train Loss: 1.32789 - Val Loss: 1.19028\n",
            "Epoch 177 - Train Loss: 1.33346 - Val Loss: 1.18816\n",
            "Epoch 178 - Train Loss: 1.32259 - Val Loss: 1.18908\n",
            "Epoch 179 - Train Loss: 1.32418 - Val Loss: 1.18937\n",
            "Epoch 180 - Train Loss: 1.32525 - Val Loss: 1.18435\n",
            "Epoch 181 - Train Loss: 1.32739 - Val Loss: 1.18510\n",
            "Epoch 182 - Train Loss: 1.32621 - Val Loss: 1.18409\n",
            "Epoch 183 - Train Loss: 1.32163 - Val Loss: 1.18293\n",
            "Epoch 184 - Train Loss: 1.32344 - Val Loss: 1.18339\n",
            "Epoch 185 - Train Loss: 1.32248 - Val Loss: 1.18219\n",
            "Epoch 186 - Train Loss: 1.31858 - Val Loss: 1.18107\n",
            "Epoch 187 - Train Loss: 1.32028 - Val Loss: 1.17979\n",
            "Epoch 188 - Train Loss: 1.31874 - Val Loss: 1.18033\n",
            "Epoch 189 - Train Loss: 1.31693 - Val Loss: 1.18085\n",
            "Epoch 190 - Train Loss: 1.30984 - Val Loss: 1.17981\n",
            "Epoch 191 - Train Loss: 1.31565 - Val Loss: 1.18018\n",
            "Epoch 192 - Train Loss: 1.31388 - Val Loss: 1.17948\n",
            "Epoch 193 - Train Loss: 1.31150 - Val Loss: 1.17905\n",
            "Epoch 194 - Train Loss: 1.30960 - Val Loss: 1.17582\n",
            "Epoch 195 - Train Loss: 1.31017 - Val Loss: 1.17586\n",
            "Epoch 196 - Train Loss: 1.30851 - Val Loss: 1.17616\n",
            "Epoch 197 - Train Loss: 1.30957 - Val Loss: 1.17571\n",
            "Epoch 198 - Train Loss: 1.30337 - Val Loss: 1.17459\n",
            "Epoch 199 - Train Loss: 1.30278 - Val Loss: 1.17364\n",
            "Epoch 200 - Train Loss: 1.30735 - Val Loss: 1.17736\n",
            "Epoch 201 - Train Loss: 1.30108 - Val Loss: 1.17161\n",
            "Epoch 202 - Train Loss: 1.29719 - Val Loss: 1.17241\n",
            "Epoch 203 - Train Loss: 1.30413 - Val Loss: 1.17036\n",
            "Epoch 204 - Train Loss: 1.30545 - Val Loss: 1.17109\n",
            "Epoch 205 - Train Loss: 1.30196 - Val Loss: 1.16760\n",
            "Epoch 206 - Train Loss: 1.29975 - Val Loss: 1.16788\n",
            "Epoch 207 - Train Loss: 1.30063 - Val Loss: 1.16802\n",
            "Epoch 208 - Train Loss: 1.30044 - Val Loss: 1.16910\n",
            "Epoch 209 - Train Loss: 1.29681 - Val Loss: 1.16991\n",
            "Epoch 210 - Train Loss: 1.29540 - Val Loss: 1.16629\n",
            "Epoch 211 - Train Loss: 1.29620 - Val Loss: 1.16908\n",
            "Epoch 212 - Train Loss: 1.29685 - Val Loss: 1.16652\n",
            "Epoch 213 - Train Loss: 1.29131 - Val Loss: 1.16474\n",
            "Epoch 214 - Train Loss: 1.29694 - Val Loss: 1.16459\n",
            "Epoch 215 - Train Loss: 1.28646 - Val Loss: 1.16563\n",
            "Epoch 216 - Train Loss: 1.28911 - Val Loss: 1.16503\n",
            "Epoch 217 - Train Loss: 1.28727 - Val Loss: 1.16436\n",
            "Epoch 218 - Train Loss: 1.28934 - Val Loss: 1.16154\n",
            "Epoch 219 - Train Loss: 1.29145 - Val Loss: 1.16441\n",
            "Epoch 220 - Train Loss: 1.29165 - Val Loss: 1.16353\n",
            "Epoch 221 - Train Loss: 1.28903 - Val Loss: 1.16223\n",
            "Epoch 222 - Train Loss: 1.28168 - Val Loss: 1.15943\n",
            "Epoch 223 - Train Loss: 1.28604 - Val Loss: 1.16078\n",
            "Epoch 224 - Train Loss: 1.28517 - Val Loss: 1.16319\n",
            "Epoch 225 - Train Loss: 1.27415 - Val Loss: 1.15912\n",
            "Epoch 226 - Train Loss: 1.28150 - Val Loss: 1.16064\n",
            "Epoch 227 - Train Loss: 1.28353 - Val Loss: 1.16088\n",
            "Epoch 228 - Train Loss: 1.27898 - Val Loss: 1.15773\n",
            "Epoch 229 - Train Loss: 1.28609 - Val Loss: 1.15440\n",
            "Epoch 230 - Train Loss: 1.27029 - Val Loss: 1.15723\n",
            "Epoch 231 - Train Loss: 1.27397 - Val Loss: 1.15781\n",
            "Epoch 232 - Train Loss: 1.27810 - Val Loss: 1.15506\n",
            "Epoch 233 - Train Loss: 1.28019 - Val Loss: 1.15633\n",
            "Epoch 234 - Train Loss: 1.27198 - Val Loss: 1.15384\n",
            "Epoch 235 - Train Loss: 1.28000 - Val Loss: 1.15441\n",
            "Epoch 236 - Train Loss: 1.27159 - Val Loss: 1.15312\n",
            "Epoch 237 - Train Loss: 1.26937 - Val Loss: 1.15057\n",
            "Epoch 238 - Train Loss: 1.27247 - Val Loss: 1.15362\n",
            "Epoch 239 - Train Loss: 1.27149 - Val Loss: 1.15545\n",
            "Epoch 240 - Train Loss: 1.26520 - Val Loss: 1.15209\n",
            "Epoch 241 - Train Loss: 1.27018 - Val Loss: 1.15325\n",
            "Epoch 242 - Train Loss: 1.27164 - Val Loss: 1.15155\n",
            "Epoch 243 - Train Loss: 1.26937 - Val Loss: 1.15052\n",
            "Epoch 244 - Train Loss: 1.26484 - Val Loss: 1.14797\n",
            "Epoch 245 - Train Loss: 1.26659 - Val Loss: 1.14885\n",
            "Epoch 246 - Train Loss: 1.26704 - Val Loss: 1.14998\n",
            "Epoch 247 - Train Loss: 1.27032 - Val Loss: 1.14790\n",
            "Epoch 248 - Train Loss: 1.26253 - Val Loss: 1.14882\n",
            "Epoch 249 - Train Loss: 1.26273 - Val Loss: 1.14832\n",
            "Epoch 250 - Train Loss: 1.26576 - Val Loss: 1.14632\n",
            "Train acc: 70.19600%\n",
            "Test acc: 58.94000%\n",
            "\n",
            "--- Entrenamiento completado. ---\n"
          ]
        }
      ]
    }
  ]
}