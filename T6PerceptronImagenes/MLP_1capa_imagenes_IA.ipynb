{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRQlk7htOOhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4378816d-44c7-4657-c021-7a464a7160f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 14 21:59:58 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "\n",
            "--- Descargando y extrayendo datos de CIFAR-10 (formato binario) ---\n",
            "Datos descargados y extraídos correctamente. Archivos disponibles en la carpeta 'cifar-10-batches-bin'.\n",
            "\n",
            "--- Clonando el repositorio GitHub con el código fuente ---\n",
            "Cloning into 'Inteligencia_Artificial'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 129 (delta 49), reused 72 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (129/129), 319.24 KiB | 11.40 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n",
            "\n",
            "--- Archivos clonados: ---\n",
            "loss_history.txt  perceptron1.cu  perceptron.cpp  plot2.cpp\n",
            "\n",
            "--- Moviendo los datos binarios de CIFAR-10 al directorio de trabajo ---\n",
            "/content/Inteligencia_Artificial/T6PerceptronImagenes\n",
            "\n",
            "--- Archivos disponibles en el directorio actual: ---\n",
            "data_batch_1.bin  data_batch_4.bin  perceptron1.cu  test_batch.bin\n",
            "data_batch_2.bin  data_batch_5.bin  perceptron.cpp\n",
            "data_batch_3.bin  loss_history.txt  plot2.cpp\n",
            "\n",
            "--- Compilando el código CUDA (perceptron1.cu) ---\n",
            "\n",
            "--- Compilación finalizada. Se generó 'perceptron_executable'. ---\n",
            "\n",
            "--- Ejecutando el programa CUDA (perceptron_executable) ---\n",
            "Cargadas 50000 train y 10000 test\n",
            "Epoch 1 - Train Loss: 0.08827 - Val Loss: 0.07943\n",
            "Epoch 2 - Train Loss: 0.07974 - Val Loss: 0.08170\n",
            "Epoch 3 - Train Loss: 0.07900 - Val Loss: 0.07958\n",
            "Epoch 4 - Train Loss: 0.07853 - Val Loss: 0.08013\n",
            "Epoch 5 - Train Loss: 0.07832 - Val Loss: 0.08033\n",
            "Epoch 6 - Train Loss: 0.07812 - Val Loss: 0.08231\n",
            "Epoch 7 - Train Loss: 0.07788 - Val Loss: 0.08181\n",
            "Epoch 8 - Train Loss: 0.07771 - Val Loss: 0.08337\n",
            "Epoch 9 - Train Loss: 0.07757 - Val Loss: 0.08160\n",
            "Epoch 10 - Train Loss: 0.07739 - Val Loss: 0.08155\n",
            "Epoch 11 - Train Loss: 0.07716 - Val Loss: 0.08122\n",
            "Epoch 12 - Train Loss: 0.07717 - Val Loss: 0.08085\n",
            "Epoch 13 - Train Loss: 0.07712 - Val Loss: 0.08204\n",
            "Epoch 14 - Train Loss: 0.07690 - Val Loss: 0.08185\n",
            "Epoch 15 - Train Loss: 0.07671 - Val Loss: 0.08190\n",
            "Epoch 16 - Train Loss: 0.07676 - Val Loss: 0.08300\n",
            "Epoch 17 - Train Loss: 0.07661 - Val Loss: 0.08306\n",
            "Epoch 18 - Train Loss: 0.07657 - Val Loss: 0.08270\n",
            "Epoch 19 - Train Loss: 0.07641 - Val Loss: 0.08194\n",
            "Epoch 20 - Train Loss: 0.07635 - Val Loss: 0.08212\n",
            "Epoch 21 - Train Loss: 0.07627 - Val Loss: 0.08195\n",
            "Epoch 22 - Train Loss: 0.07620 - Val Loss: 0.08243\n",
            "Epoch 23 - Train Loss: 0.07611 - Val Loss: 0.08227\n",
            "Epoch 24 - Train Loss: 0.07604 - Val Loss: 0.08283\n",
            "Epoch 25 - Train Loss: 0.07587 - Val Loss: 0.08200\n",
            "Epoch 26 - Train Loss: 0.07598 - Val Loss: 0.08344\n",
            "Epoch 27 - Train Loss: 0.07581 - Val Loss: 0.08346\n",
            "Epoch 28 - Train Loss: 0.07576 - Val Loss: 0.08246\n",
            "Epoch 29 - Train Loss: 0.07564 - Val Loss: 0.08192\n",
            "Epoch 30 - Train Loss: 0.07568 - Val Loss: 0.08260\n",
            "Epoch 31 - Train Loss: 0.07556 - Val Loss: 0.08343\n",
            "Epoch 32 - Train Loss: 0.07557 - Val Loss: 0.08498\n",
            "Epoch 33 - Train Loss: 0.07553 - Val Loss: 0.08256\n",
            "Epoch 34 - Train Loss: 0.07536 - Val Loss: 0.08443\n",
            "Epoch 35 - Train Loss: 0.07535 - Val Loss: 0.08339\n",
            "Epoch 36 - Train Loss: 0.07532 - Val Loss: 0.08306\n",
            "Epoch 37 - Train Loss: 0.07516 - Val Loss: 0.08244\n",
            "Epoch 38 - Train Loss: 0.07522 - Val Loss: 0.08319\n",
            "Epoch 39 - Train Loss: 0.07526 - Val Loss: 0.08364\n",
            "Epoch 40 - Train Loss: 0.07519 - Val Loss: 0.08434\n",
            "Epoch 41 - Train Loss: 0.07503 - Val Loss: 0.08332\n",
            "Epoch 42 - Train Loss: 0.07501 - Val Loss: 0.08396\n",
            "Epoch 43 - Train Loss: 0.07509 - Val Loss: 0.08246\n",
            "Epoch 44 - Train Loss: 0.07499 - Val Loss: 0.08494\n",
            "Epoch 45 - Train Loss: 0.07490 - Val Loss: 0.08315\n",
            "Epoch 46 - Train Loss: 0.07489 - Val Loss: 0.08402\n",
            "Epoch 47 - Train Loss: 0.07488 - Val Loss: 0.08343\n",
            "Epoch 48 - Train Loss: 0.07478 - Val Loss: 0.08369\n",
            "Epoch 49 - Train Loss: 0.07479 - Val Loss: 0.08279\n",
            "Epoch 50 - Train Loss: 0.07480 - Val Loss: 0.08315\n",
            "Epoch 51 - Train Loss: 0.07468 - Val Loss: 0.08454\n",
            "Epoch 52 - Train Loss: 0.07474 - Val Loss: 0.08458\n",
            "Epoch 53 - Train Loss: 0.07457 - Val Loss: 0.08397\n",
            "Epoch 54 - Train Loss: 0.07455 - Val Loss: 0.08374\n",
            "Epoch 55 - Train Loss: 0.07449 - Val Loss: 0.08347\n",
            "Epoch 56 - Train Loss: 0.07449 - Val Loss: 0.08455\n",
            "Epoch 57 - Train Loss: 0.07451 - Val Loss: 0.08319\n",
            "Epoch 58 - Train Loss: 0.07442 - Val Loss: 0.08342\n",
            "Epoch 59 - Train Loss: 0.07436 - Val Loss: 0.08682\n",
            "Epoch 60 - Train Loss: 0.07441 - Val Loss: 0.08371\n",
            "Epoch 61 - Train Loss: 0.07431 - Val Loss: 0.08442\n",
            "Epoch 62 - Train Loss: 0.07435 - Val Loss: 0.08421\n",
            "Epoch 63 - Train Loss: 0.07428 - Val Loss: 0.08552\n",
            "Epoch 64 - Train Loss: 0.07430 - Val Loss: 0.08350\n",
            "Epoch 65 - Train Loss: 0.07423 - Val Loss: 0.08445\n",
            "Epoch 66 - Train Loss: 0.07423 - Val Loss: 0.08421\n",
            "Epoch 67 - Train Loss: 0.07410 - Val Loss: 0.08378\n",
            "Epoch 68 - Train Loss: 0.07414 - Val Loss: 0.08427\n",
            "Epoch 69 - Train Loss: 0.07412 - Val Loss: 0.08493\n",
            "Epoch 70 - Train Loss: 0.07404 - Val Loss: 0.08431\n",
            "Epoch 71 - Train Loss: 0.07401 - Val Loss: 0.08390\n",
            "Epoch 72 - Train Loss: 0.07405 - Val Loss: 0.08329\n",
            "Epoch 73 - Train Loss: 0.07395 - Val Loss: 0.08701\n",
            "Epoch 74 - Train Loss: 0.07397 - Val Loss: 0.08409\n",
            "Epoch 75 - Train Loss: 0.07391 - Val Loss: 0.08428\n",
            "Epoch 76 - Train Loss: 0.07398 - Val Loss: 0.08512\n",
            "Epoch 77 - Train Loss: 0.07392 - Val Loss: 0.08515\n",
            "Epoch 78 - Train Loss: 0.07397 - Val Loss: 0.08442\n",
            "Epoch 79 - Train Loss: 0.07391 - Val Loss: 0.08397\n",
            "Epoch 80 - Train Loss: 0.07381 - Val Loss: 0.08463\n",
            "Epoch 81 - Train Loss: 0.07381 - Val Loss: 0.08493\n",
            "Epoch 82 - Train Loss: 0.07375 - Val Loss: 0.08490\n",
            "Epoch 83 - Train Loss: 0.07385 - Val Loss: 0.08385\n",
            "Epoch 84 - Train Loss: 0.07366 - Val Loss: 0.08417\n",
            "Epoch 85 - Train Loss: 0.07381 - Val Loss: 0.08639\n",
            "Epoch 86 - Train Loss: 0.07372 - Val Loss: 0.08491\n",
            "Epoch 87 - Train Loss: 0.07363 - Val Loss: 0.08604\n",
            "Epoch 88 - Train Loss: 0.07367 - Val Loss: 0.08472\n",
            "Epoch 89 - Train Loss: 0.07360 - Val Loss: 0.08621\n",
            "Epoch 90 - Train Loss: 0.07353 - Val Loss: 0.08489\n",
            "Epoch 91 - Train Loss: 0.07353 - Val Loss: 0.08456\n",
            "Epoch 92 - Train Loss: 0.07348 - Val Loss: 0.08491\n",
            "Epoch 93 - Train Loss: 0.07353 - Val Loss: 0.08364\n",
            "Epoch 94 - Train Loss: 0.07348 - Val Loss: 0.08488\n",
            "Epoch 95 - Train Loss: 0.07345 - Val Loss: 0.08511\n",
            "Epoch 96 - Train Loss: 0.07349 - Val Loss: 0.08549\n",
            "Epoch 97 - Train Loss: 0.07345 - Val Loss: 0.08616\n",
            "Epoch 98 - Train Loss: 0.07339 - Val Loss: 0.08535\n",
            "Epoch 99 - Train Loss: 0.07341 - Val Loss: 0.08530\n",
            "Epoch 100 - Train Loss: 0.07331 - Val Loss: 0.08481\n",
            "Train acc: 47.23000%\n",
            "Test acc: 36.73000%\n",
            "\n",
            "--- Entrenamiento completado. ---\n"
          ]
        }
      ],
      "source": [
        "# PASO 1: Comprobar GPU y entorno CUDA\n",
        "\n",
        "# Verificar si la GPU está habilitada en este Colab\n",
        "!nvidia-smi\n",
        "\n",
        "# Verificar que Colab tiene los controladores y herramientas de CUDA necesarias instaladas\n",
        "!nvcc --version\n",
        "\n",
        "\n",
        "# PASO 2: Descargar y preparar los datos CIFAR-10 (formato binario)\n",
        "print(\"\\n--- Descargando y extrayendo datos de CIFAR-10 (formato binario) ---\")\n",
        "\n",
        "# Descargar el archivo tar.gz directamente desde la página oficial del CIFAR-10\n",
        "!wget -q -c https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
        "\n",
        "# Extraer el archivo en el directorio actual\n",
        "!tar -xzf cifar-10-binary.tar.gz\n",
        "\n",
        "print(\"Datos descargados y extraídos correctamente. Archivos disponibles en la carpeta 'cifar-10-batches-bin'.\")\n",
        "\n",
        "# PASO 3: Clonar el repositorio GitHub para obtener el código CUDA\n",
        "\n",
        "print(\"\\n--- Clonando el repositorio GitHub con el código fuente ---\")\n",
        "!git clone https://github.com/aaabriceno/Inteligencia_Artificial.git\n",
        "\n",
        "# Verificar que el repositorio se clonó correctamente\n",
        "print(\"\\n--- Archivos clonados: ---\")\n",
        "!ls Inteligencia_Artificial/T6PerceptronImagenes/\n",
        "\n",
        "# PASO 4: Copiar los datos de CIFAR-10 al directorio del código CUDA\n",
        "\n",
        "print(\"\\n--- Moviendo los datos binarios de CIFAR-10 al directorio de trabajo ---\")\n",
        "\n",
        "# Copiar los archivos .bin de \"cifar-10-batches-bin\" al directorio donde está el archivo .cu\n",
        "!cp cifar-10-batches-bin/*.bin Inteligencia_Artificial/T6PerceptronImagenes/\n",
        "\n",
        "# Cambiar el directorio de trabajo al del archivo CUDA\n",
        "%cd Inteligencia_Artificial/T6PerceptronImagenes/\n",
        "\n",
        "# Confirmar que los datos están en el directorio correcto\n",
        "print(\"\\n--- Archivos disponibles en el directorio actual: ---\")\n",
        "!ls\n",
        "\n",
        "# PASO 5: Compilar el código CUDA usando nvcc\n",
        "\n",
        "print(\"\\n--- Compilando el código CUDA (perceptron1.cu) ---\")\n",
        "\n",
        "# Usar el compilador nvcc para compilar el archivo\n",
        "!nvcc perceptron1.cu -o perceptron_executable -arch=sm_75\n",
        "\n",
        "print(\"\\n--- Compilación finalizada. Se generó 'perceptron_executable'. ---\")\n",
        "\n",
        "# PASO 6: Ejecutar el programa compilado\n",
        "print(\"\\n--- Ejecutando el programa CUDA (perceptron_executable) ---\")\n",
        "\n",
        "# Ejecutar el archivo compilado\n",
        "!./perceptron_executable\n",
        "\n",
        "print(\"\\n--- Entrenamiento completado. ---\")\n"
      ]
    }
  ]
}